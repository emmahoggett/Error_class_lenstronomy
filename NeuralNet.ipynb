{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "NeuralNet.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "behind-benchmark"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmahoggett/Error_class_lenstronomy/blob/master/NeuralNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Fl8vga6I15K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b09d5cf-fe4c-48d4-ccee-8f51744adc1c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0,'/content/drive/My Drive/Colab Notebooks/deeplens/')"
      ],
      "id": "6Fl8vga6I15K",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.activity.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fphotos.native&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "4/1AY0e-g4FxL5QRsmrGZzI2Qn3ECaMQ-cKsdwORPuUtqTqXc59D8WGNwFgoHk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XmINXYhI3Z8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff358a1d-1c64-4076-d5f6-2e77040494a5"
      },
      "source": [
        "!pip3 install torch==1.2.0+cu92 torchvision==0.4.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip3 install lenstronomy\n",
        "!pip3 install deeplenstronomy\n",
        "!pip install h5py\n",
        "!pip install matplotlib==3.1.3\n",
        "!pip install --upgrade tables"
      ],
      "id": "5XmINXYhI3Z8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.2.0+cu92\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu92/torch-1.2.0%2Bcu92-cp37-cp37m-manylinux1_x86_64.whl (663.1MB)\n",
            "\u001b[K     |████████████████████████████████| 663.1MB 25kB/s \n",
            "\u001b[?25hCollecting torchvision==0.4.0+cu92\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu92/torchvision-0.4.0%2Bcu92-cp37-cp37m-manylinux1_x86_64.whl (8.8MB)\n",
            "\u001b[K     |████████████████████████████████| 8.8MB 33.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.2.0+cu92) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.4.0+cu92) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchvision==0.4.0+cu92) (1.15.0)\n",
            "\u001b[31mERROR: torchtext 0.9.0 has requirement torch==1.8.0, but you'll have torch 1.2.0+cu92 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.8.0+cu101\n",
            "    Uninstalling torch-1.8.0+cu101:\n",
            "      Successfully uninstalled torch-1.8.0+cu101\n",
            "  Found existing installation: torchvision 0.9.0+cu101\n",
            "    Uninstalling torchvision-0.9.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.9.0+cu101\n",
            "Successfully installed torch-1.2.0+cu92 torchvision-0.4.0+cu92\n",
            "Collecting lenstronomy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/83/de461ea68a52ed70df2457d8995e589053d2b1289bcedb852fcaa174dc63/lenstronomy-1.7.0.tar.gz (440kB)\n",
            "\u001b[K     |████████████████████████████████| 440kB 4.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.7/dist-packages (from lenstronomy) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from lenstronomy) (1.4.1)\n",
            "Collecting configparser\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Building wheels for collected packages: lenstronomy\n",
            "  Building wheel for lenstronomy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lenstronomy: filename=lenstronomy-1.7.0-cp37-none-any.whl size=632852 sha256=3fdcce30d272a987d1a9d11109aade0e7d99930203c353729ca92dd61d12016a\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/36/eb/a1bbf501dc372452b939df565ca4da0b5d0f1001b05739ae46\n",
            "Successfully built lenstronomy\n",
            "Installing collected packages: configparser, lenstronomy\n",
            "Successfully installed configparser-5.0.2 lenstronomy-1.7.0\n",
            "Collecting deeplenstronomy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4f/44/af23f21637c343e71fd0b1d9d4eedd940f8ff55264690f36b6c9a7ccdf33/deeplenstronomy-0.0.1.5-py2.py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.7MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: lenstronomy>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from deeplenstronomy) (1.7.0)\n",
            "Collecting matplotlib>=3.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/3d/db9a6b3c83c9511301152dbb64a029c3a4313c86eaef12c237b13ecf91d6/matplotlib-3.3.4-cp37-cp37m-manylinux1_x86_64.whl (11.5MB)\n",
            "\u001b[K     |████████████████████████████████| 11.6MB 32.8MB/s \n",
            "\u001b[?25hCollecting scipy>=1.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/3a/9e0649ab2d5ade703baa70ef980aa08739226e5d6a642f084bb201a92fc2/scipy-1.6.1-cp37-cp37m-manylinux1_x86_64.whl (27.4MB)\n",
            "\u001b[K     |█████████████████▊              | 15.2MB 33.8MB/s eta 0:00:01"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "written-sample"
      },
      "source": [
        "## 0. Import"
      ],
      "id": "written-sample"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "comfortable-tucson"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import PIL\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "from helpers import*\n",
        "from model.baseline import*\n",
        "from model.densenet121 import*\n",
        "from model.alexnet import*\n",
        "from model.resnet18 import*\n",
        "from model.vgg16 import*"
      ],
      "id": "comfortable-tucson",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enhanced-enterprise"
      },
      "source": [
        "## 1. Building the data set"
      ],
      "id": "enhanced-enterprise"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "intermediate-blood",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "187c13e4-d0fc-4823-99c2-b00b571b08dd"
      },
      "source": [
        "# Build the four classes \n",
        "config_repo_model = 'drive/My Drive/Colab Notebooks/deeplens/data/configFile/config_model'\n",
        "\n",
        "\n",
        "for i in np.arange(1,4):\n",
        "    model_name = config_repo_model + str(i) + '.yaml'\n",
        "    res = Residual(model_name)\n",
        "    res.build(i)\n",
        "\n",
        "print('Data Generation Finished')"
      ],
      "id": "intermediate-blood",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Generation Finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4ngxjJlpBa7"
      },
      "source": [
        "metadata = pd.DataFrame()\r\n",
        "for i in np.arange(1,4):\r\n",
        "    [img, meta] = read_hdf5(i)\r\n",
        "    metadata = pd.concat([metadata,meta], ignore_index=True)\r\n",
        "    if i == 1:\r\n",
        "        final_array = img\r\n",
        "    else:\r\n",
        "         final_array = np.concatenate((final_array, img))\r\n",
        "\r\n",
        "data_set = CombineDataset(metadata,'ID','class',final_array)"
      ],
      "id": "j4ngxjJlpBa7",
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "6idI_Ve2jfvf",
        "outputId": "3c682a01-eb90-4147-9b04-c9f3aceafb5b"
      },
      "source": [
        "class CombineDataset(Dataset):\r\n",
        "    \"\"\"\r\n",
        "    This class helps us to build a pytorch tensor by combining the images and the\r\n",
        "    metadata.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, frame, id_col, label_name, images, nb_channel = 1):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            frame (pd.DataFrame): Frame with the tabular data.\r\n",
        "            id_col (string): Name of the column that connects image to tabular data\r\n",
        "            label_name (string): Name of the column with the label to be predicted\r\n",
        "            path_imgs (string): Path to the folder where the images are.\r\n",
        "            nb_channel (int): Number of channels.\r\n",
        "        \"\"\"\r\n",
        "        self.frame = frame\r\n",
        "        self.id_col = id_col\r\n",
        "        self.label_name = label_name\r\n",
        "        self.images = images\r\n",
        "        self.nb_channel = nb_channel\r\n",
        "        \r\n",
        "    def __len__(self):\r\n",
        "        return (self.frame.shape[0])\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        if torch.is_tensor(idx):\r\n",
        "            idx = idx.tolist()\r\n",
        "        #complete image path and read\r\n",
        "        img_name = self.frame[self.id_col].iloc[idx]\r\n",
        "        image = image[img_name]\r\n",
        "        image = torch.from_numpy(image.astype(np.float64))\r\n",
        "\r\n",
        "        #get the other features to be used as training data\r\n",
        "        feats = [feat for feat in self.frame.columns if feat not in [self.label_name,self.id_col]]\r\n",
        "        feats  = np.array(self.frame[feats].iloc[idx])\r\n",
        "        feats = torch.from_numpy(feats.astype(np.float64))\r\n",
        "       \r\n",
        "        \r\n",
        "        #get label\r\n",
        "        label = np.array(self.frame[self.label_name].iloc[idx])\r\n",
        "        label = torch.from_numpy(label.astype(np.float64))\r\n",
        "\r\n",
        "        return image, feats, label\r\n",
        "        \r\n",
        "    def size(self):\r\n",
        "        return (self.frame.shape[0])"
      ],
      "id": "6idI_Ve2jfvf",
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-3aa6b675b5a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'CombineDataset' object has no attribute 'images'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UkL0TbTeB092",
        "outputId": "63962a04-6b32-4152-94f8-1ee6c075aaec"
      },
      "source": [
        "data_train, data_test = train_test_split(data_set,train_size=0.9,\r\n",
        "                                         random_state=42)"
      ],
      "id": "UkL0TbTeB092",
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'ID'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-e887b3d19b7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m data_train, data_test = train_test_split(data_set,train_size=0.9,\n\u001b[0;32m----> 2\u001b[0;31m                                          random_state=42)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m     return list(chain.from_iterable((_safe_indexing(a, train),\n\u001b[0;32m-> 2146\u001b[0;31m                                      _safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[1;32m   2147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m     return list(chain.from_iterable((_safe_indexing(a, train),\n\u001b[0;32m-> 2146\u001b[0;31m                                      _safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[1;32m   2147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_safe_indexing\u001b[0;34m(X, indices, axis)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_array_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_list_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_list_indexing\u001b[0;34m(X, key, key_dtype)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;31m# key is a integer array-like of key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;31m# key is a integer array-like of key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/Colab Notebooks/deeplens/helpers.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;31m#complete image path and read\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mimg_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'ID'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZedkskkOGjZ",
        "outputId": "c1728fb5-4f81-4954-86fd-a0cd0fc6d811"
      },
      "source": [
        "img_name = metadata['ID'].iloc[1]\r\n",
        "print(img_name)"
      ],
      "id": "dZedkskkOGjZ",
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veterinary-tiffany",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "67c88f81-0f08-4a99-9c6f-33736712c327"
      },
      "source": [
        "# Open and concatenate the metadata in one data frame\n",
        "data_repo = '/content/drive/My Drive/Colab Notebooks/deeplens/data/dataSet/'\n",
        "metadata = pd.DataFrame()\n",
        "for i in np.arange(1,4):\n",
        "    file_name = data_repo+'MetaE'+str(i)+'.csv'\n",
        "    meta = pd.read_csv(file_name)\n",
        "    metadata = pd.concat([metadata,meta], ignore_index=True)\n",
        "\n",
        "\n",
        "\n",
        "# Combine the residuals with the metadata in a pytorch tensor\n",
        "data_set = CombineDataset(metadata,'ID','class',data_repo)\n",
        "\n",
        "data_train, data_test = train_test_split(data_set,train_size=0.9,\n",
        "                                         random_state=42)\n",
        "\n",
        "loader_train = DataLoader(data_train, batch_size = 4, shuffle = True, \n",
        "                          num_workers = 0, drop_last=True)\n",
        "loader_test = DataLoader(data_test, batch_size = 4, shuffle = True, \n",
        "                         num_workers = 0, drop_last=True)"
      ],
      "id": "veterinary-tiffany",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-19fb184095ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m data_train, data_test = train_test_split(data_set,train_size=0.9, \n\u001b[0;32m---> 15\u001b[0;31m                                                  random_state=42)\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m loader_train = DataLoader(data_train, batch_size = 4, shuffle = True, \n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m     return list(chain.from_iterable((_safe_indexing(a, train),\n\u001b[0;32m-> 2146\u001b[0;31m                                      _safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[1;32m   2147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m     return list(chain.from_iterable((_safe_indexing(a, train),\n\u001b[0;32m-> 2146\u001b[0;31m                                      _safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[1;32m   2147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_safe_indexing\u001b[0;34m(X, indices, axis)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_array_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_list_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_list_indexing\u001b[0;34m(X, key, key_dtype)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;31m# key is a integer array-like of key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;31m# key is a integer array-like of key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/Colab Notebooks/deeplens/helpers.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfeat\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mfeats\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'PIXEL'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1hXSMRVNN4p",
        "outputId": "d541c3d9-f917-41fd-f052-dc672000c417"
      },
      "source": [
        "data_set"
      ],
      "id": "B1hXSMRVNN4p",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method CombineDataset.size of <helpers.CombineDataset object at 0x7f9bec9c2b10>>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy5xVr3LN4kn"
      },
      "source": [
        "loader_train = DataLoader(data_train, batch_size = 4, shuffle = True, \r\n",
        "                          num_workers = 0, drop_last=True)\r\n",
        "loader_test = DataLoader(data_test, batch_size = 4, shuffle = True, \r\n",
        "                         num_workers = 0, drop_last=True)"
      ],
      "id": "wy5xVr3LN4kn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "behind-benchmark"
      },
      "source": [
        "## 2. Building basic neural network"
      ],
      "id": "behind-benchmark"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRnYdliz-86r"
      },
      "source": [
        "#### 2.1. Tabular network"
      ],
      "id": "jRnYdliz-86r"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "american-direction"
      },
      "source": [
        "net = TabularNetBasic()\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "id": "american-direction",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "black-clock",
        "outputId": "ae5e7d0d-dd11-46b8-9696-a38df6f96756"
      },
      "source": [
        "for epoch in range(40):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(loader_train, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, meta_inputs,labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(meta_inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "id": "black-clock",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Qk3EtWlgn5E",
        "outputId": "ef9adc72-d90a-42ed-95d2-a8075ddb2a91"
      },
      "source": [
        "f1_samples = 0\r\n",
        "accuracy = 0\r\n",
        "iteration = 0\r\n",
        "with torch.no_grad():\r\n",
        "    predictions = []\r\n",
        "    targets = []\r\n",
        "    for data in loader_test:\r\n",
        "        images, meta_img, labels = data\r\n",
        "        outputs = net(meta_img)\r\n",
        "        predictions.extend(outputs.cpu().numpy())\r\n",
        "        targets.extend(labels.cpu().numpy())\r\n",
        "        result = calculate_metrics(np.round(np.array(predictions)), np.array(targets))\r\n",
        "\r\n",
        "        f1_samples+=result['samples/f1']\r\n",
        "        accuracy+=result['samples/recall']\r\n",
        "        iteration+=1\r\n",
        "\r\n",
        "\r\n",
        "mean_f1 = f1_samples/(iteration+1)\r\n",
        "mean_accuracy =accuracy/(iteration+1)\r\n",
        "\r\n",
        "print(\"iter:{:3d} \\n test:\\n \"\r\n",
        "        \"accuracy: {:.3f} \"\r\n",
        "        \"samples f1: {:.3f}\".format(iteration,\r\n",
        "                                    mean_accuracy,\r\n",
        "                                    mean_f1))"
      ],
      "id": "7Qk3EtWlgn5E",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter: 30 \n",
            " test:\n",
            " accuracy: 0.638 samples f1: 0.650\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKbsptH2_qTa"
      },
      "source": [
        "#### 2.2. Residual image "
      ],
      "id": "PKbsptH2_qTa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Epv5bSQ8_2v6"
      },
      "source": [
        "net = CNNNetBasic()\r\n",
        "criterion = nn.BCELoss()\r\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "id": "Epv5bSQ8_2v6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jdGxSbwAkOV",
        "outputId": "460a1c21-eb0d-4464-e2fe-8ba53fc99986"
      },
      "source": [
        "for epoch in range(40):  # loop over the dataset multiple times\r\n",
        "\r\n",
        "    running_loss = 0.0\r\n",
        "    for i, data in enumerate(loader_train, 0):\r\n",
        "        # get the inputs; data is a list of [inputs, labels]\r\n",
        "        inputs, meta_inputs,labels = data\r\n",
        "\r\n",
        "        # zero the parameter gradients\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        # forward + backward + optimize\r\n",
        "        outputs = net(inputs)\r\n",
        "        loss = criterion(outputs, labels)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        # print statistics\r\n",
        "        running_loss += loss.item()\r\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\r\n",
        "            print('[%d, %5d] loss: %.3f' %\r\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\r\n",
        "            running_loss = 0.0\r\n",
        "\r\n",
        "print('Finished Training')"
      ],
      "id": "6jdGxSbwAkOV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9chu_f9-Ao6m",
        "outputId": "e9587de9-93a2-4065-fc17-f57f2ef27da6"
      },
      "source": [
        "f1_samples = 0\r\n",
        "accuracy = 0\r\n",
        "iteration = 0\r\n",
        "with torch.no_grad():\r\n",
        "    predictions = []\r\n",
        "    targets = []\r\n",
        "    for data in loader_test:\r\n",
        "        images, meta_img, labels = data\r\n",
        "        outputs = net(images)\r\n",
        "        predictions.extend(outputs.cpu().numpy())\r\n",
        "        targets.extend(labels.cpu().numpy())\r\n",
        "        result = calculate_metrics(np.round(np.array(predictions)), np.array(targets))\r\n",
        "\r\n",
        "        f1_samples+=result['samples/f1']\r\n",
        "        accuracy+=result['samples/recall']\r\n",
        "        iteration+=1\r\n",
        "\r\n",
        "\r\n",
        "mean_f1 = f1_samples/(iteration+1)\r\n",
        "mean_accuracy =accuracy/(iteration+1)\r\n",
        "\r\n",
        "print(\"iter:{:3d} \\n test:\\n \"\r\n",
        "        \"accuracy: {:.3f} \"\r\n",
        "        \"samples f1: {:.3f}\".format(iteration,\r\n",
        "                                    mean_accuracy,\r\n",
        "                                    mean_f1))"
      ],
      "id": "9chu_f9-Ao6m",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter: 30 \n",
            " test:\n",
            " accuracy: 0.590 samples f1: 0.593\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg6vJz4yAxHq"
      },
      "source": [
        "#### 2.3. Residual and tabular data"
      ],
      "id": "Mg6vJz4yAxHq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCRVuISBA4EI"
      },
      "source": [
        "net = TabularCNNNetBasic()\r\n",
        "criterion = nn.BCELoss()\r\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "id": "FCRVuISBA4EI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zv4or2J2BBmR",
        "outputId": "0f626671-a27f-45bb-9ef5-b47cbe9333d4"
      },
      "source": [
        "for epoch in range(35):  # loop over the dataset multiple times\r\n",
        "\r\n",
        "    running_loss = 0.0\r\n",
        "    for i, data in enumerate(loader_train, 0):\r\n",
        "        # get the inputs; data is a list of [inputs, labels]\r\n",
        "        inputs, meta_inputs,labels = data\r\n",
        "\r\n",
        "        # zero the parameter gradients\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        # forward + backward + optimize\r\n",
        "        outputs = net(inputs,meta_inputs)\r\n",
        "        loss = criterion(outputs, labels)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        # print statistics\r\n",
        "        running_loss += loss.item()\r\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\r\n",
        "            print('[%d, %5d] loss: %.3f' %\r\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\r\n",
        "            running_loss = 0.0\r\n",
        "\r\n",
        "print('Finished Training')"
      ],
      "id": "Zv4or2J2BBmR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJYuS6dbBJ9y",
        "outputId": "e140b4b5-f62e-4e21-8eb4-4ad2f6e663a0"
      },
      "source": [
        "f1_samples = 0\r\n",
        "accuracy = 0\r\n",
        "iteration = 0\r\n",
        "with torch.no_grad():\r\n",
        "    predictions = []\r\n",
        "    targets = []\r\n",
        "    for data in loader_test:\r\n",
        "        images, meta_img, labels = data\r\n",
        "        outputs = net(images,meta_img)\r\n",
        "        predictions.extend(outputs.cpu().numpy())\r\n",
        "        targets.extend(labels.cpu().numpy())\r\n",
        "        result = calculate_metrics(np.round(np.array(predictions)), np.array(targets))\r\n",
        "\r\n",
        "        f1_samples+=result['samples/f1']\r\n",
        "        accuracy+=result['samples/recall']\r\n",
        "        iteration+=1\r\n",
        "\r\n",
        "\r\n",
        "mean_f1 = f1_samples/(iteration+1)\r\n",
        "mean_accuracy =accuracy/(iteration+1)\r\n",
        "\r\n",
        "print(\"iter:{:3d} \\n test:\\n \"\r\n",
        "        \"accuracy: {:.3f} \"\r\n",
        "        \"samples f1: {:.3f}\".format(iteration,\r\n",
        "                                    mean_accuracy,\r\n",
        "                                    mean_f1))"
      ],
      "id": "wJYuS6dbBJ9y",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter: 30 \n",
            " test:\n",
            " accuracy: 0.744 samples f1: 0.749\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFggBB71DUYd"
      },
      "source": [
        "## 3. Building different model"
      ],
      "id": "fFggBB71DUYd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKMUD771DhqL"
      },
      "source": [
        "#### 3.1. AlexNet"
      ],
      "id": "oKMUD771DhqL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIDkMax0Dg7b"
      },
      "source": [
        "squeezenet = models.squeezenet1_0()\r\n",
        "inception = models.inception_v3()\r\n",
        "shufflenet = models.shufflenet_v2_x1_0()\r\n",
        "mobilenet = models.mobilenet_v2()\r\n",
        "resnext50_32x4d = models.resnext50_32x4d()\r\n",
        "wide_resnet50_2 = models.wide_resnet50_2()\r\n",
        "mnasnet = models.mnasnet1_0()"
      ],
      "id": "OIDkMax0Dg7b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv3shmIVPD5-"
      },
      "source": [
        "###### 3.1.1. Residual maps"
      ],
      "id": "Qv3shmIVPD5-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5s3kMmVv_1U"
      },
      "source": [
        "net = AlexNetResidual()\r\n",
        "criterion = nn.BCELoss()\r\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "id": "D5s3kMmVv_1U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f0hNnGbswQOC",
        "outputId": "283b1c9b-ce3e-4095-dd78-fae48b22f00e"
      },
      "source": [
        "for epoch in range(20):  # loop over the dataset multiple times\r\n",
        "\r\n",
        "    running_loss = 0.0\r\n",
        "    for i, data in enumerate(loader_train, 0):\r\n",
        "        # get the inputs; data is a list of [inputs, labels]\r\n",
        "        inputs, meta_inputs,labels = data\r\n",
        "        inputs = inputs.resize_([4,1,224,224])\r\n",
        "        # zero the parameter gradients\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        # forward + backward + optimize\r\n",
        "        outputs = net(inputs)\r\n",
        "        loss = criterion(outputs, labels)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        # print statistics\r\n",
        "        running_loss += loss.item()\r\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\r\n",
        "            print('[%d, %5d] loss: %.3f' %\r\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\r\n",
        "            running_loss = 0.0\r\n",
        "\r\n",
        "print('Finished Training')"
      ],
      "id": "f0hNnGbswQOC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uz9vNVCD9tWA"
      },
      "source": [
        "f1_samples = 0\r\n",
        "accuracy = 0\r\n",
        "iteration = 0\r\n",
        "with torch.no_grad():\r\n",
        "    predictions = []\r\n",
        "    targets = []\r\n",
        "    for data in loader_test:\r\n",
        "        images, meta_img, labels = data\r\n",
        "        images = images.resize_([4,1,224,224])\r\n",
        "        outputs = net(images)\r\n",
        "        predictions.extend(outputs.cpu().numpy())\r\n",
        "        targets.extend(labels.cpu().numpy())\r\n",
        "        result = calculate_metrics(np.round(np.array(predictions)), np.array(targets))\r\n",
        "\r\n",
        "        f1_samples+=result['samples/f1']\r\n",
        "        accuracy+=result['samples/recall']\r\n",
        "        iteration+=1\r\n",
        "\r\n",
        "\r\n",
        "mean_f1 = f1_samples/(iteration+1)\r\n",
        "mean_accuracy =accuracy/(iteration+1)\r\n",
        "\r\n",
        "print(\"iter:{:3d} \\n test:\\n \"\r\n",
        "        \"accuracy: {:.3f} \"\r\n",
        "        \"samples f1: {:.3f}\".format(iteration,\r\n",
        "                                    mean_accuracy,\r\n",
        "                                    mean_f1))"
      ],
      "id": "uz9vNVCD9tWA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmBn3d5GO9Ee"
      },
      "source": [
        "###### 3.1.2. Residual maps & Metadata\r\n"
      ],
      "id": "OmBn3d5GO9Ee"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb0IdvH7Sa9c"
      },
      "source": [
        "net = AlexNetResidualMetadata()\r\n",
        "criterion = nn.BCELoss()\r\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "id": "Tb0IdvH7Sa9c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDLg5KaNSe1p"
      },
      "source": [
        "for epoch in range(20):  # loop over the dataset multiple times\r\n",
        "\r\n",
        "    running_loss = 0.0\r\n",
        "    for i, data in enumerate(loader_train, 0):\r\n",
        "        # get the inputs; data is a list of [inputs, labels]\r\n",
        "        inputs, meta_inputs,labels = data\r\n",
        "        inputs = inputs.resize_([4,1,224,224])\r\n",
        "        # zero the parameter gradients\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        # forward + backward + optimize\r\n",
        "        outputs = net(inputs, meta_inputs)\r\n",
        "        loss = criterion(outputs, labels)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        # print statistics\r\n",
        "        running_loss += loss.item()\r\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\r\n",
        "            print('[%d, %5d] loss: %.3f' %\r\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\r\n",
        "            running_loss = 0.0\r\n",
        "\r\n",
        "print('Finished Training')"
      ],
      "id": "FDLg5KaNSe1p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxS8DlpxSq7B",
        "outputId": "582cfa7b-0c01-4b65-a8c4-aa8e5730ce57"
      },
      "source": [
        "f1_samples = 0\r\n",
        "accuracy = 0\r\n",
        "iteration = 0\r\n",
        "with torch.no_grad():\r\n",
        "    predictions = []\r\n",
        "    targets = []\r\n",
        "    for data in loader_test:\r\n",
        "        images, meta_img, labels = data\r\n",
        "        images = images.resize_([4,1,224,224])\r\n",
        "        outputs = net(images,meta_img)\r\n",
        "        predictions.extend(outputs.cpu().numpy())\r\n",
        "        targets.extend(labels.cpu().numpy())\r\n",
        "        result = calculate_metrics(np.round(np.array(predictions)), np.array(targets))\r\n",
        "\r\n",
        "        f1_samples+=result['samples/f1']\r\n",
        "        accuracy+=result['samples/recall']\r\n",
        "        iteration+=1\r\n",
        "\r\n",
        "\r\n",
        "mean_f1 = f1_samples/(iteration+1)\r\n",
        "mean_accuracy =accuracy/(iteration+1)\r\n",
        "\r\n",
        "print(\"iter:{:3d} \\n test:\\n \"\r\n",
        "        \"accuracy: {:.3f} \"\r\n",
        "        \"samples f1: {:.3f}\".format(iteration,\r\n",
        "                                    mean_accuracy,\r\n",
        "                                    mean_f1))"
      ],
      "id": "vxS8DlpxSq7B",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter: 29 \n",
            " test:\n",
            " accuracy: 0.467 samples f1: 0.474\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5xXe9fZUvcV"
      },
      "source": [
        "#### 3.2. Resnet18"
      ],
      "id": "r5xXe9fZUvcV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8LHel77VHRt"
      },
      "source": [
        "###### 3.2.1. Residual maps"
      ],
      "id": "f8LHel77VHRt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Hr1w_7s0Now"
      },
      "source": [
        "net = resnet18maps(1,3)\r\n",
        "criterion = nn.BCELoss()\r\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "id": "1Hr1w_7s0Now",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LMIVBqx0ROP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d669e0c-b0e2-47fa-de9c-9ce7414904eb"
      },
      "source": [
        "for epoch in range(20):  # loop over the dataset multiple times\r\n",
        "\r\n",
        "    running_loss = 0.0\r\n",
        "    for i, data in enumerate(loader_train, 0):\r\n",
        "        # get the inputs; data is a list of [inputs, labels]\r\n",
        "        inputs, meta_inputs,labels = data\r\n",
        "        inputs = inputs.resize_([4,1,224,224])\r\n",
        "        # zero the parameter gradients\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        # forward + backward + optimize\r\n",
        "        outputs = net(inputs)\r\n",
        "        loss = criterion(outputs, labels)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        # print statistics\r\n",
        "        running_loss += loss.item()\r\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\r\n",
        "            print('[%d, %5d] loss: %.3f' %\r\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\r\n",
        "            running_loss = 0.0\r\n",
        "\r\n",
        "print('Finished Training')"
      ],
      "id": "6LMIVBqx0ROP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6axEfVW00Kj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45585d50-21f5-41f6-97ad-0f3fa22fd29f"
      },
      "source": [
        "f1_samples = 0\r\n",
        "accuracy = 0\r\n",
        "iteration = 0\r\n",
        "with torch.no_grad():\r\n",
        "    predictions = []\r\n",
        "    targets = []\r\n",
        "    for data in loader_test:\r\n",
        "        images, meta_img, labels = data\r\n",
        "        images = images.resize_([4,1,224,224])\r\n",
        "        outputs = net(images)\r\n",
        "        predictions.extend(outputs.cpu().numpy())\r\n",
        "        targets.extend(labels.cpu().numpy())\r\n",
        "        result = calculate_metrics(np.round(np.array(predictions)), np.array(targets))\r\n",
        "\r\n",
        "        f1_samples+=result['samples/f1']\r\n",
        "        accuracy+=result['samples/recall']\r\n",
        "        iteration+=1\r\n",
        "\r\n",
        "\r\n",
        "mean_f1 = f1_samples/(iteration+1)\r\n",
        "mean_accuracy =accuracy/(iteration+1)\r\n",
        "\r\n",
        "print(\"iter:{:3d} \\n test:\\n \"\r\n",
        "        \"accuracy: {:.3f} \"\r\n",
        "        \"samples f1: {:.3f}\".format(iteration,\r\n",
        "                                    mean_accuracy,\r\n",
        "                                    mean_f1))"
      ],
      "id": "6axEfVW00Kj_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter: 29 \n",
            " test:\n",
            " accuracy: 0.105 samples f1: 0.119\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAcj1hnaVQiD"
      },
      "source": [
        "###### 3.2.2. Residual maps & Metadata"
      ],
      "id": "nAcj1hnaVQiD"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loGuiCpfXx4M"
      },
      "source": [
        "net = resnet18meta(1,7,3)\r\n",
        "criterion = nn.BCELoss()\r\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "id": "loGuiCpfXx4M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZyDYzr2YZFS",
        "outputId": "40c28644-9a69-40bb-f495-cb3768a0045b"
      },
      "source": [
        "for epoch in range(20):  # loop over the dataset multiple times\r\n",
        "\r\n",
        "    running_loss = 0.0\r\n",
        "    for i, data in enumerate(loader_train, 0):\r\n",
        "        # get the inputs; data is a list of [inputs, labels]\r\n",
        "        inputs, meta_inputs,labels = data\r\n",
        "        inputs = inputs.resize_([4,1,224,224])\r\n",
        "        # zero the parameter gradients\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        # forward + backward + optimize\r\n",
        "        outputs = net(inputs, meta_inputs)\r\n",
        "        loss = criterion(outputs, labels)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        # print statistics\r\n",
        "        running_loss += loss.item()\r\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\r\n",
        "            print('[%d, %5d] loss: %.3f' %\r\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\r\n",
        "            running_loss = 0.0\r\n",
        "\r\n",
        "print('Finished Training')"
      ],
      "id": "JZyDYzr2YZFS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1esRElEX0Mbo",
        "outputId": "e6abdeb5-083d-4dee-d7cb-bed713445b72"
      },
      "source": [
        "f1_samples = 0\r\n",
        "accuracy = 0\r\n",
        "iteration = 0\r\n",
        "with torch.no_grad():\r\n",
        "    predictions = []\r\n",
        "    targets = []\r\n",
        "    for data in loader_test:\r\n",
        "        images, meta_img, labels = data\r\n",
        "        images = images.resize_([4,1,224,224])\r\n",
        "        outputs = net(images,meta_img)\r\n",
        "        predictions.extend(outputs.cpu().numpy())\r\n",
        "        targets.extend(labels.cpu().numpy())\r\n",
        "        result = calculate_metrics(np.round(np.array(predictions)), np.array(targets))\r\n",
        "\r\n",
        "        f1_samples+=result['samples/f1']\r\n",
        "        accuracy+=result['samples/recall']\r\n",
        "        iteration+=1\r\n",
        "\r\n",
        "\r\n",
        "mean_f1 = f1_samples/(iteration+1)\r\n",
        "mean_accuracy =accuracy/(iteration+1)\r\n",
        "\r\n",
        "print(\"iter:{:3d} \\n test:\\n \"\r\n",
        "        \"accuracy: {:.3f} \"\r\n",
        "        \"samples f1: {:.3f}\".format(iteration,\r\n",
        "                                    mean_accuracy,\r\n",
        "                                    mean_f1))"
      ],
      "id": "1esRElEX0Mbo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter: 29 \n",
            " test:\n",
            " accuracy: 0.652 samples f1: 0.667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PpS3OPlWL1i"
      },
      "source": [
        "#### 3.3. VGG16"
      ],
      "id": "0PpS3OPlWL1i"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flUuQXTgWTdI"
      },
      "source": [
        " ###### 3.3.1. Residual maps"
      ],
      "id": "flUuQXTgWTdI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWiuE5X51TkG"
      },
      "source": [
        "net = VGG16Residual()\r\n",
        "criterion = nn.BCELoss()\r\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "id": "jWiuE5X51TkG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBZBvZ6a1Wvv"
      },
      "source": [
        "for epoch in range(20):  # loop over the dataset multiple times\r\n",
        "\r\n",
        "    running_loss = 0.0\r\n",
        "    for i, data in enumerate(loader_train, 0):\r\n",
        "        # get the inputs; data is a list of [inputs, labels]\r\n",
        "        inputs, meta_inputs,labels = data\r\n",
        "        inputs = inputs.resize_([4,1,224,224])\r\n",
        "        # zero the parameter gradients\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        # forward + backward + optimize\r\n",
        "        outputs = net(inputs)\r\n",
        "        loss = criterion(outputs, labels)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        # print statistics\r\n",
        "        running_loss += loss.item()\r\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\r\n",
        "            print('[%d, %5d] loss: %.3f' %\r\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\r\n",
        "            running_loss = 0.0\r\n",
        "\r\n",
        "print('Finished Training')"
      ],
      "id": "gBZBvZ6a1Wvv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ewush4uecUku"
      },
      "source": [
        "f1_samples = 0\r\n",
        "accuracy = 0\r\n",
        "iteration = 0\r\n",
        "with torch.no_grad():\r\n",
        "    predictions = []\r\n",
        "    targets = []\r\n",
        "    for data in loader_test:\r\n",
        "        images, meta_img, labels = data\r\n",
        "        images = images.resize_([4,1,224,224])\r\n",
        "        outputs = net(images)\r\n",
        "        predictions.extend(outputs.cpu().numpy())\r\n",
        "        targets.extend(labels.cpu().numpy())\r\n",
        "        result = calculate_metrics(np.round(np.array(predictions)), np.array(targets))\r\n",
        "\r\n",
        "        f1_samples+=result['samples/f1']\r\n",
        "        accuracy+=result['samples/recall']\r\n",
        "        iteration+=1\r\n",
        "\r\n",
        "\r\n",
        "mean_f1 = f1_samples/(iteration+1)\r\n",
        "mean_accuracy =accuracy/(iteration+1)\r\n",
        "\r\n",
        "print(\"iter:{:3d} \\n test:\\n \"\r\n",
        "        \"accuracy: {:.3f} \"\r\n",
        "        \"samples f1: {:.3f}\".format(iteration,\r\n",
        "                                    mean_accuracy,\r\n",
        "                                    mean_f1))"
      ],
      "id": "Ewush4uecUku",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1URgP6vW1pyV"
      },
      "source": [
        "###### 3.3.2. Residual maps & Metadata"
      ],
      "id": "1URgP6vW1pyV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7p5y5jIcCYf"
      },
      "source": [
        "net = VGG16ResidualMetadata()\r\n",
        "criterion = nn.BCELoss()\r\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "id": "T7p5y5jIcCYf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYjwIbRRcCMw"
      },
      "source": [
        "for epoch in range(20):  # loop over the dataset multiple times\r\n",
        "\r\n",
        "    running_loss = 0.0\r\n",
        "    for i, data in enumerate(loader_train, 0):\r\n",
        "        # get the inputs; data is a list of [inputs, labels]\r\n",
        "        inputs, meta_inputs,labels = data\r\n",
        "        inputs = inputs.resize_([4,1,224,224])\r\n",
        "        # zero the parameter gradients\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        # forward + backward + optimize\r\n",
        "        outputs = net(inputs,meta_inputs)\r\n",
        "        loss = criterion(outputs, labels)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        # print statistics\r\n",
        "        running_loss += loss.item()\r\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\r\n",
        "            print('[%d, %5d] loss: %.3f' %\r\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\r\n",
        "            running_loss = 0.0\r\n",
        "\r\n",
        "print('Finished Training')"
      ],
      "id": "DYjwIbRRcCMw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DYujbuHcXj3"
      },
      "source": [
        "f1_samples = 0\r\n",
        "accuracy = 0\r\n",
        "iteration = 0\r\n",
        "with torch.no_grad():\r\n",
        "    predictions = []\r\n",
        "    targets = []\r\n",
        "    for data in loader_test:\r\n",
        "        images, meta_img, labels = data\r\n",
        "        images = images.resize_([4,1,224,224])\r\n",
        "        outputs = net(images,meta_img)\r\n",
        "        predictions.extend(outputs.cpu().numpy())\r\n",
        "        targets.extend(labels.cpu().numpy())\r\n",
        "        result = calculate_metrics(np.round(np.array(predictions)), np.array(targets))\r\n",
        "\r\n",
        "        f1_samples+=result['samples/f1']\r\n",
        "        accuracy+=result['samples/recall']\r\n",
        "        iteration+=1\r\n",
        "\r\n",
        "\r\n",
        "mean_f1 = f1_samples/(iteration+1)\r\n",
        "mean_accuracy =accuracy/(iteration+1)\r\n",
        "\r\n",
        "print(\"iter:{:3d} \\n test:\\n \"\r\n",
        "        \"accuracy: {:.3f} \"\r\n",
        "        \"samples f1: {:.3f}\".format(iteration,\r\n",
        "                                    mean_accuracy,\r\n",
        "                                    mean_f1))"
      ],
      "id": "2DYujbuHcXj3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66C5ftu6cfFP"
      },
      "source": [
        "#### 3.4. Dense Net 121"
      ],
      "id": "66C5ftu6cfFP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFOigMqbdD-z"
      },
      "source": [
        "###### 3.4.1. Residual maps"
      ],
      "id": "dFOigMqbdD-z"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTX_xhrfZ0wW"
      },
      "source": [
        "net = densenet121()\r\n",
        "criterion = nn.BCELoss()\r\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "id": "jTX_xhrfZ0wW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gV19-p4jau9u"
      },
      "source": [
        "for epoch in range(20):  # loop over the dataset multiple times\r\n",
        "\r\n",
        "    running_loss = 0.0\r\n",
        "    for i, data in enumerate(loader_train, 0):\r\n",
        "        # get the inputs; data is a list of [inputs, labels]\r\n",
        "        inputs, meta_inputs,labels = data\r\n",
        "        inputs = inputs.resize_([4,1,224,224])\r\n",
        "        # zero the parameter gradients\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        # forward + backward + optimize\r\n",
        "        outputs = net(inputs)\r\n",
        "        loss = criterion(outputs, labels)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        # print statistics\r\n",
        "        running_loss += loss.item()\r\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\r\n",
        "            print('[%d, %5d] loss: %.3f' %\r\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\r\n",
        "            running_loss = 0.0\r\n",
        "\r\n",
        "print('Finished Training')"
      ],
      "id": "gV19-p4jau9u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOcti75Qlfd1"
      },
      "source": [
        "f1_samples = 0\r\n",
        "accuracy = 0\r\n",
        "iteration = 0\r\n",
        "with torch.no_grad():\r\n",
        "    predictions = []\r\n",
        "    targets = []\r\n",
        "    for data in loader_test:\r\n",
        "        images, meta_img, labels = data\r\n",
        "        images = images.resize_([4,1,224,224])\r\n",
        "        outputs = net(images)\r\n",
        "        predictions.extend(outputs.cpu().numpy())\r\n",
        "        targets.extend(labels.cpu().numpy())\r\n",
        "        result = calculate_metrics(np.round(np.array(predictions)), np.array(targets))\r\n",
        "\r\n",
        "        f1_samples+=result['samples/f1']\r\n",
        "        accuracy+=result['samples/recall']\r\n",
        "        iteration+=1\r\n",
        "\r\n",
        "\r\n",
        "mean_f1 = f1_samples/(iteration+1)\r\n",
        "mean_accuracy =accuracy/(iteration+1)\r\n",
        "\r\n",
        "print(\"iter:{:3d} \\n test:\\n \"\r\n",
        "        \"accuracy: {:.3f} \"\r\n",
        "        \"samples f1: {:.3f}\".format(iteration,\r\n",
        "                                    mean_accuracy,\r\n",
        "                                    mean_f1))"
      ],
      "id": "qOcti75Qlfd1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc3nOViRdQnZ"
      },
      "source": [
        "###### 3.4.2. Residual maps & Metadata"
      ],
      "id": "Uc3nOViRdQnZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQksvp5XfF3j"
      },
      "source": [
        ""
      ],
      "id": "qQksvp5XfF3j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFEJTNK8cug8"
      },
      "source": [
        "#### 3.5. Google Net"
      ],
      "id": "RFEJTNK8cug8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSxdPLAmdIxT"
      },
      "source": [
        "###### 3.5.1. Residual maps"
      ],
      "id": "cSxdPLAmdIxT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLURFuUAfG3a"
      },
      "source": [
        ""
      ],
      "id": "dLURFuUAfG3a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znRrFM6kdTLx"
      },
      "source": [
        "###### 3.5.2. Residual maps & Metadata"
      ],
      "id": "znRrFM6kdTLx"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uq6vAbqEfHSK"
      },
      "source": [
        ""
      ],
      "id": "uq6vAbqEfHSK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL0p7kKwc0CU"
      },
      "source": [
        "#### 3.6. Squeeze Net 1_0"
      ],
      "id": "FL0p7kKwc0CU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSAM99yvdK8i"
      },
      "source": [
        "###### 3.6.1. Residual maps"
      ],
      "id": "TSAM99yvdK8i"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3gMHYPdfH7C"
      },
      "source": [
        ""
      ],
      "id": "o3gMHYPdfH7C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxaBKRoXdV6g"
      },
      "source": [
        "###### 3.6.2. Residual maps & Metadata"
      ],
      "id": "XxaBKRoXdV6g"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GS8V0LYfI8a"
      },
      "source": [
        ""
      ],
      "id": "7GS8V0LYfI8a",
      "execution_count": null,
      "outputs": []
    }
  ]
}