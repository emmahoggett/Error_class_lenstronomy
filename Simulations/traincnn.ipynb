{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "contrary-richardson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from helpers.data_generation.file_management import read_hdf5\n",
    "from helpers.data_generation.error_generation import Residual, CombineDataset\n",
    "from helpers.model.helpers_model import NeuralNet\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-discount",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.75\n",
    "percent = np.array([0.005, 0.015, 0.005])\n",
    "size = 600\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "res = Residual()\n",
    "res.build(size, ratio = ratio, per_error = percent)\n",
    "print('Data building finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-recognition",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_ID =  \"S\"+str(size)+\"R\"+str(int(ratio*100))\n",
    "[final_array, metadata] = read_hdf5(str_ID)\n",
    "metadata ['ID'] = np.arange(0,final_array.shape[0])\n",
    "\n",
    "data_set = CombineDataset(metadata,'ID','class',final_array)\n",
    "\n",
    "data_train, data_test = train_test_split(data_set,train_size=0.85,random_state=42)\n",
    "\n",
    "batch_size = 50; max_epoch = 50\n",
    "\n",
    "\n",
    "loader_train = DataLoader(data_train, batch_size = batch_size, num_workers = 8, drop_last=True, pin_memory = True)\n",
    "loader_test = DataLoader(data_test, batch_size = batch_size, num_workers = 8, drop_last=True, pin_memory = True)\n",
    "\n",
    "print('Reading Data Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-institution",
   "metadata": {},
   "source": [
    "## 2. Convolutionnal neural network (CNN)\n",
    "### 2.0 Baseline\n",
    "\n",
    "Results :\n",
    "* Baseline : Epoch : - AUCROC :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-teach",
   "metadata": {},
   "outputs": [],
   "source": [
    "netbasic = NeuralNet('BasicCNN', 'SGD/momentum')\n",
    "#epoch: 2.000, auc: 0.845\n",
    "#netbasic.load_checkpoint('_current')\n",
    "while netbasic.current_epoch < max_epoch:\n",
    "    netbasic.train(loader_train)\n",
    "    res = netbasic.test(loader_test)\n",
    "    print(\"epoch \"+str(epoch))\n",
    "\n",
    "print('Finished Training : AlexNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-worse",
   "metadata": {},
   "source": [
    "### 2.1. Spatial exploitation CNN\n",
    "\n",
    "Results :\n",
    "* AlexNet : Epoch : 17 - AUCROC : 0.974 - AUCROC Mass : 0.998 - AUCROC Source 0.949\n",
    "* VGG16 : Epoch : - AUCROC : \n",
    "* GoogleNet : Epoch : - AUCROC : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "netbasic = NeuralNet('AlexNet', 'SGD/momentum')\n",
    "#epoch: 2.000, auc: 0.845\n",
    "netbasic.load_checkpoint('_current')\n",
    "while netbasic.current_epoch < max_epoch:\n",
    "    netbasic.train(loader_train)\n",
    "    res = netbasic.test(loader_test)\n",
    "    print(\"epoch \"+str(epoch))\n",
    "\n",
    "print('Finished Training : AlexNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "netbasic = NeuralNet('VGG16', 'SGD/momentum')\n",
    "#epoch: 2.000, auc: 0.845\n",
    "netbasic.load_checkpoint('_current')\n",
    "while netbasic.current_epoch < max_epoch:\n",
    "    netbasic.train(loader_train)\n",
    "    res = netbasic.test(loader_test)\n",
    "    print(\"epoch \"+str(epoch))\n",
    "\n",
    "print('Finished Training : VGG16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-tampa",
   "metadata": {},
   "outputs": [],
   "source": [
    "netbasic = NeuralNet('GoogleNet', 'SGD/momentum')\n",
    "#epoch: 2.000, auc: 0.845\n",
    "netbasic.load_checkpoint('_current')\n",
    "while netbasic.current_epoch < max_epoch:\n",
    "    netbasic.train(loader_train)\n",
    "    res = netbasic.test(loader_test)\n",
    "    print(\"epoch \"+str(epoch))\n",
    "\n",
    "print('Finished Training : GoogleNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-merchant",
   "metadata": {},
   "source": [
    "### 2.2. Multi-path exploitation CNN\n",
    "\n",
    "Results :\n",
    "* ResNet18 : Epoch : - AUCROC : \n",
    "* DenseNet161 : Epoch : - AUCROC : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-great",
   "metadata": {},
   "outputs": [],
   "source": [
    "netbasic = NeuralNet('ResNet18', 'SGD/momentum')\n",
    "#epoch: 2.000, auc: 0.845\n",
    "netbasic.load_checkpoint('_current')\n",
    "while netbasic.current_epoch < max_epoch:\n",
    "    netbasic.train(loader_train)\n",
    "    res = netbasic.test(loader_test)\n",
    "    print(\"epoch \"+str(epoch))\n",
    "\n",
    "print('Finished Training : ResNet18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-dryer",
   "metadata": {},
   "outputs": [],
   "source": [
    "netbasic = NeuralNet('DenseNet161', 'SGD/momentum')\n",
    "#epoch: 2.000, auc: 0.845\n",
    "netbasic.load_checkpoint('_current')\n",
    "while netbasic.current_epoch < max_epoch:\n",
    "    netbasic.train(loader_train)\n",
    "    res = netbasic.test(loader_test)\n",
    "    print(\"epoch \"+str(epoch))\n",
    "\n",
    "print('Finished Training : DenseNet161')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-muscle",
   "metadata": {},
   "source": [
    "### 2.3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "netbasic = NeuralNet('SqueezeNet', 'SGD/momentum')\n",
    "#epoch: 2.000, auc: 0.845\n",
    "netbasic.load_checkpoint('_current')\n",
    "while netbasic.current_epoch < max_epoch:\n",
    "    netbasic.train(loader_train)\n",
    "    res = netbasic.test(loader_test)\n",
    "    print(\"epoch \"+str(epoch))\n",
    "\n",
    "print('Finished Training : SqueezeNet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
