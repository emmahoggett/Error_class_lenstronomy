#  this file contains classes that build the residual maps with lenstronomy

import numpy as np
import pandas as pd
from sklearn.preprocessing import MultiLabelBinarizer
from itertools import combinations

from lenstronomy.LensModel.Profiles.pemd import PEMD
from lenstronomy.SimulationAPI.ObservationConfig.HST import HST
from astropy.io import fits
from lenstronomy.SimulationAPI.sim_api import SimAPI

from helpers import store_hdf5
from errors import LensMassError, SourceError


    
class LensDataset:
    """
    Class that build residual and metadata in lenstronomy. This class configure the observation setting and use a psf file in the
    Time Delay Lens Modeling Challenge in the Rung0 training exercise.
        path : data/rung0/code1/f160w-seed3/drizzled_image/psf.fits
    
    Data is generated over the HST model with kwargs model such that:
            - Lens Mass model : PEMD
            - Source model    : SERSIC_ELLIPSE
    """
    def __init__(self, size:int, seed:int = 0, center_x:float = 0, center_y:float = 0, mass_range = None, source_range = None):
        """Initialization of the object LensDataset.
        
        
        :param size       : dataset size
        :param seed       : random seed for the uniform distribution
        :param center_x   : coordinate center of the lens mass profile on the x-axis
        :param center_y   : coordinate center of the lens mass profile on the y-axis
        :param param_range: 11x2 matrix, with specified range of the mass and source light profile
        """

        #define the psf and the type of observation (here HST)
        psf_hdu = fits.open('data/psf.fits')
        kwargs_hst = HST().kwargs_single_band()
        kwargs_hst['kernel_point_source'] = np.array(psf_hdu[0].data)
        numPix = 64 

        kwargs_model = {'lens_model_list': ['PEMD'],                    #list of lens models to be used
                        'source_light_model_list': ['SERSIC_ELLIPSE'],  #list of extended source models to be used
                       }
        
        self.size = size
        self.image_config = SimAPI(numpix=numPix, kwargs_single_band=kwargs_hst, kwargs_model=kwargs_model)
        self.img_sim = self.image_config.image_model_class()
        
        
        #build dictionnary for the lens mass and the source model
        self.mass_error = LensMassError(size,seed, center_x, center_y, mass_range)
        self.source_error = SourceError(size,seed,source_range)
        
        self.metadata = pd.concat([self.mass_error.metadata,self.source_error.metadata], axis =1)
        self.images = np.zeros((self.size, 1,64,64))
        
        for i in np.arange(0,self.size): 
            kwargs_lens = self.mass_error.get_kwargs(i)
            kwargs_source = self.source_error.get_kwargs(i)

            #generate image
            self.images[i,:,:] = self.img_sim.image(kwargs_lens, kwargs_source, kwargs_ps=None)
        
    
class  Residual:
    """
    Class that build the residual maps and the metadata. This dataset ends up in the folder data/dataSet by default.
    The data set is generated by the lenstronomy simulators. Error combinations are generated through a dictionnary.
    
    The file ID is coded as following : P[percent_error]R[ratio]_lens.h5 for the residual maps and P[percent_error]R[ratio]_meta.h5 
    for the metadata.
    """

    def build(self, size:int, ratio:float = 0.75, per_error:float = 0.01, num_label:int = 2, center_x:float = 0, center_y:float = 0,
              mass_range = None, source_range = None, path_data:str = "data/dataSet/"):
        """
        
        :param size         : int, size of the final data set
        :param ratio        : float, ratio between true and false residuals maps
        :param per_error    : float, percentage of error between observed data and the model
        :param num_label    : int, number of labels
        :param center_x     : float, x-position of the lens center - default : 0
        :param center_y     : float, y-position of the lens center - default : 0
        :param mass_range   : np.array (4,2), array that contain the distribution parameters of each configuration in the following order :
                    - theta_E : Einstein radius(angle), log-normal distribution [mu,sigma]- default : N(0,0.1)
                    - gamma   : Logarithmic slope of the power-law profile, log-normal distribution [mu,sigma]- default : N(0.7,0.1)
                    - q       : Axis ratio, uniform distribution [a,b] - default : U(0.7,1)
                    - phi     : Position angle, uniform distribution [a,b] - default : U(0,pi/2)
        :param source_range : np.array (7,2), array that contain the distribution parameters of each source configurationin the following order :
                - amp      : Surface brightness/amplitude value at the half light radius, uniform distribution [a,b] - default : U(20,24)
                - R        : Semi-major axis half light radius, log-normal distribution [mu,sigma]- default : N(-0.7,0.4)
                - n        : Sersic index, log-normal distribution [mu,sigma]- default : N(0.7,0.4)
                - center_x : x-position of the source center, uniform distribution [a,b] - default : U(-0.5,0.5)
                - center_y : y-position of the source center, uniform distribution [a,b] - default : U(-0.5,0.5)
                - q        : Axis ratio, uniform distribution [a,b] - default : U(0.7,1)
                - phi      : Position angle, uniform distribution [a,b] - default : U(0,pi/2)
        :param path_data    : str, path where the binary file is saved - default : "data/dataSet/"
        """
        
        metadata = pd.DataFrame()
        self.per_error = per_error
        bool_mdimg = []
        k = 0
        count_size = 0
        self.path_data = path_data
        self.size = round(size/(2**num_label-1))
        dataset_model = LensDataset(size = size*(2**num_label-1), center_x = center_x, center_y = center_y, mass_range = mass_range, source_range= source_range)

        self.channels = dataset_model.images.shape[1]
        residuals = np.zeros((size, self.channels,64,64), dtype=np.half)
        
        #build residuals for each combination of error
        for errorID in range(2**num_label-1):
            np.random.seed(errorID*10)
            false_idx = np.array([], dtype='int')
            true_idx = np.array([], dtype='int')
            
            #build residuals for a model that fit the observation
            while true_idx.shape[0]<((1-ratio)*self.size) and count_size <= size:
                r=np.random.randint(0,size-1)
                if r not in true_idx: 
                    count_size +=1
                    true_idx = np.append(true_idx, r)
                    
            for i in true_idx:
                for i_ch in np.arange(0,self.channels):
                    image_model = dataset_model.images[i][i_ch]
                    image_real = dataset_model.images[i][i_ch] + dataset_model.image_config.noise_for_model(model = dataset_model.images[i][i_ch])
                    sigma = np.sqrt(dataset_model.img_sim.Data.C_D_model(model = dataset_model.images[i][i_ch]))
                    residuals[k,i_ch,:,:] = (image_real-image_model)/sigma

                bool_mdimg.append({})
                metadata = pd.concat([metadata,dataset_model.metadata.take([i])])
                k +=1
                
            
            #build residuals for a model that doesn't fit the observation    
            while false_idx.shape[0]<(self.size-true_idx.shape[0]) or (count_size < size and errorID == 2**num_label-2):
                r=np.random.randint(0,size-1)
                if r not in false_idx: 
                    count_size +=1
                    false_idx = np.append(false_idx, r)

            img_test = np.zeros((self.channels,64,64))  
            for i in false_idx:
                dict_err = self.dict_error(dataset_model, i)
                kwargs_lens, kwargs_source, error =  dict_err[str(errorID)]
                img_test = dataset_model.img_sim.image(kwargs_lens, kwargs_source, kwargs_ps=None)

                for i_ch in np.arange(0,self.channels):
                    image_model = dataset_model.images[i][i_ch]
                    image_real = img_test + dataset_model.image_config.noise_for_model(model = img_test)
                    sigma = np.sqrt(dataset_model.img_sim.Data.C_D_model(model = img_test))
                    residuals[k,i_ch,:,:] = (image_real-image_model)/sigma
                bool_mdimg.append(error)
                metadata = pd.concat([metadata,dataset_model.metadata.take([i])])
                k = k+1

            
        self.residuals = residuals; self.metadata = metadata
        
        #add the labels for each model and store the dataset in .h5 file    
        #use a multi label encoder - error is defined as a set of string
        mlb = MultiLabelBinarizer()
        bool_mdimg = mlb.fit_transform(bool_mdimg)
        metadata['class'] = np.reshape(bool_mdimg, (-1, num_label)).tolist()
        
        #save the dataset
        ID_img = "P"+str(int(self.per_error*100))+"R"+str(int(ratio*100))
        store_hdf5(residuals, metadata, ID_img, path = self.path_data)
        
        
    def dict_error(self, dataset_model, i):
        """
        
        :param dataset_model : LensDataset, object that contain all image configuration and images
        :param i             : int, index of the used image
        :return              : dictionnary, with every type of error combinations. One combination is a tuple with the parameters and an error set.
        """
        
        #list of possible errors
        error_list = [dataset_model.mass_error.type_error, dataset_model.source_error.type_error]
        kwargs_err = [dataset_model.mass_error.add_error(i,self.per_error), dataset_model.source_error.add_error(i,self.per_error)]
        
        #build all possible combinations of error 
        err = []
        for L in range(1,len(error_list)+1):
            err = err + list(combinations(error_list, L))
            
        err_set = [set(t) for t in err]
        kwargs = []
        dict_err = {}
        
        #build a dictionnary for the image i with every type of error
        for k, err_i in enumerate(err_set):
            kwargs_k =[dataset_model.mass_error.get_kwargs(i), dataset_model.source_error.get_kwargs(i)]
            
            for j, error_list_j in enumerate(error_list):
                if error_list_j in err_i:
                    kwargs_k[j] = kwargs_err[j]
            
            kwargs_k.append(err_i)
            dict_err[str(k)] = tuple(kwargs_k)

        return dict_err
        
        
